{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# JSN_XGBoost_machine\n",
    "Training with numerical data JSN prediction\n",
    "\n",
    "\n",
    "author = MV<br>\n",
    "date = 2021-11-04<br>\n",
    "\n",
    "_______________________________________\n",
    "\n",
    "description"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this defines the GPU you are using\n",
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"   # see issue #152\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add paths for dnn2 and labelbox-connector\n",
    "import sys\n",
    "sys.path.insert(1, \"/srv/dnn-framework\")\n",
    "sys.path.insert(1, \"/srv/labelbox-connector\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#general\n",
    "import matplotlib.pylab as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import logging\n",
    "import cv2\n",
    "import copy\n",
    "\n",
    "#tensorflow\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dnn2 framework\n",
    "from framework.dnn_app.dnn_app import DnnApp\n",
    "from framework.data_objects import *\n",
    "from framework.dataset.dataset_collection import DatasetCollection\n",
    "from framework.dataset.csv_data_helper import CsvDataHelper\n",
    "from framework.batch_generator.batch_generator import BatchGenerator\n",
    "from framework.models.model_generator import ModelGenerator\n",
    "from framework.trainings_scheduler.trainings_scheduler import TrainingsScheduler\n",
    "\n",
    "# dnn2 networks\n",
    "from networks.models.ModelTypes import ModelTypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries that will be needed for the lab\n",
    "\n",
    "import xgboost as xgb\n",
    "import numpy as np\n",
    "from collections import OrderedDict\n",
    "import gc\n",
    "from glob import glob\n",
    "import os\n",
    "import pandas as pd\n",
    "from copy import copy\n",
    "from time import time\n",
    "from sklearn.metrics import roc_auc_score,confusion_matrix,accuracy_score,classification_report,roc_curve\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from timeit import default_timer\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "\n",
    "\n",
    "\n",
    "# Set the seed for numpy\n",
    "np.random.seed(123)\n",
    "\n",
    "#general\n",
    "import matplotlib.pylab as plt\n",
    "import matplotlib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import logging\n",
    "import cv2\n",
    "import albumentations as A\n",
    "\n",
    "#dnn\n",
    "import keras.backend as K\n",
    "import tensorflow as tf\n",
    "from keras.callbacks import ModelCheckpoint, ReduceLROnPlateau, TensorBoard, EarlyStopping\n",
    "from keras.optimizers import Adam\n",
    "from keras.layers import Conv2D\n",
    "from keras.models import Model, load_model\n",
    "\n",
    "#framework\n",
    "from framework.utils.app_base import DnnApp\n",
    "from framework.utils import Landmarks, BoundingBox, img_utils\n",
    "from framework.loader import LmBlobBatchGenerator\n",
    "from framework.dataset import DatasetAugmented\n",
    "from framework.preprocessing import preprocessing, image_processing\n",
    "from framework.augmentor import AugmentorImage\n",
    "\n",
    "from imgaug import augmenters as iaa\n",
    "from sklearn.metrics import confusion_matrix, mean_squared_error, cohen_kappa_score, roc_auc_score, roc_curve, log_loss\n",
    "from sklearn.metrics import precision_recall_fscore_support, classification_report\n",
    "import itertools\n",
    "\n",
    "from datetime import date\n",
    "\n",
    "import keras\n",
    "from keras.applications.resnet50 import ResNet50\n",
    "from keras.models import Model\n",
    "\n",
    "from network.iblmodel import IBLModel as Model\n",
    "from network.RetinaNet.resnet import ResNet2D50\n",
    "from keras.layers import Input, Dense, LeakyReLU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#configs\n",
    "%matplotlib inline\n",
    "logging.basicConfig(format='%(asc' 'time)s %(name)-25s %(level' 'name)-8s %(message)s')\n",
    "logging.getLogger().setLevel(logging.INFO) # you change this to logging.DEBUG to get more logging information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#session config - if needed\n",
    "import tensorflow.keras.backend as K\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.per_process_gpu_memory_fraction = 0.4\n",
    "K.set_session(tf.Session(config=config))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalize data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Add knee injection to JSN20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mod = pd.read_csv(\"/srv/XGBoost/Master_dataset_JSN.csv\")\n",
    "#mod = mod[['Knee_inj', 'Laterality','ID']]\n",
    "mod.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = df.merge(mod, on =['Laterality', 'ID'], how = 'left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "\n",
    "test = test.replace({'Knee_inj': {'No':0, 'Yes':1, '1: Yes': 1, '0: No':0}})\n",
    "\n",
    "## StandardScaler\n",
    "col_names = ['Knee_inj']\n",
    "new_col_names = ['Knee_inj_S']\n",
    "features = test[col_names]\n",
    "\n",
    "scaler = preprocessing.StandardScaler().fit(features.values)\n",
    "features = scaler.transform(features.values)\n",
    "scaled_features = pd.DataFrame(features, columns = new_col_names)\n",
    "scaled_features['ID'] = test.ID\n",
    "scaled_features['Laterality'] = test.Laterality\n",
    "\n",
    "scaled_features = scaled_features.dropna()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = '/srv/XGBoost/Master_10JSN_XML_ex014.csv'\n",
    "df =  pd.read_csv(data_path)\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "from sklearn import preprocessing\n",
    "\n",
    "## StandardScaler\n",
    "col_names = ['Patient Sex','Patient Age','BMI','Hip_symptoms','WOMAC_dis','WOMAC_stiff','WOMAC_pain','other_knee_KOA','KL-grade_a','sclerosis_a','osteophytes_a']\n",
    "new_col_names = ['Patient Sex_S','Patient Age_S','BMI_S','Hip_symptoms_S','WOMAC_dis_S','WOMAC_stiff_S','WOMAC_pain_S','other_knee_KOA_S','KL-grade_a_S','sclerosis_a_S','osteophytes_a_S']\n",
    "features = df[col_names]\n",
    "\n",
    "scaler = preprocessing.StandardScaler().fit(features.values)\n",
    "features = scaler.transform(features.values)\n",
    "scaled_features = pd.DataFrame(features, columns = new_col_names)\n",
    "\n",
    "## eliminate all nan values\n",
    "#scaled_features = scaled_features.dropna()\n",
    "\n",
    "scaled_features['ID'] = df.ID\n",
    "scaled_features['Laterality'] = df.Laterality\n",
    "scaled_features['class']= df['class']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.merge(scaled_features, on = ['Laterality','ID'], how = 'left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## drop all undesired columns\n",
    "df = df.drop(columns={'KL', 'Implant','dicom_img_path.1'})\n",
    "print(len(df))\n",
    "df = df.dropna()\n",
    "print(len(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(df['class'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# here we train a label encoder so that we can map our classes to integers later for model training\n",
    "le = LabelEncoder()\n",
    "le.fit(df['class'])\n",
    "print(le.classes_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split into Train and test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## For standard scaler data\n",
    "df_num = df[[\n",
    "    \"Patient Age_S\",\n",
    "    'BMI_S',\n",
    "    \"Patient Sex_S\",\n",
    "    \"Hip_symptoms_S\",\n",
    "    \"Knee_inj_S\",\n",
    "    \"WOMAC_dis_S\",\n",
    "    \"WOMAC_pain_S\",\n",
    "    \"WOMAC_stiff_S\",\n",
    "    \"sclerosis_S\",\n",
    "    \"osteophytes_S\",\n",
    "    \"KL-grade_S\",\n",
    "    'other_knee_KOA_S',\n",
    "    'class']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## For original data\n",
    "df_num = df[[\n",
    "    \"Patient Age\",\n",
    "    'BMI',\n",
    "    \"Patient Sex\",\n",
    "    \"Hip_symptoms\",\n",
    "    'Knee_inj',\n",
    "    \"WOMAC_dis\",\n",
    "    \"WOMAC_pain\",\n",
    "    \"WOMAC_stiff\",\n",
    "    \"KL-grade_a\",\n",
    "    \"sclerosis_a\",\n",
    "    \"osteophytes_a\",\n",
    "    'other_knee_KOA',\n",
    "    'class']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# capture the labels\n",
    "labels = df['class'].copy()\n",
    "\n",
    "# split data into test and train\n",
    "x_train, x_test, y_train, y_test = train_test_split(df_num,\n",
    "                                                    labels,\n",
    "                                                    test_size=.25, \n",
    "                                                    random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check that the dimensions of our train and test sets are okay\n",
    "print(x_train.shape)\n",
    "print('Number of slow progressors in Train set:',len(x_train.loc[x_train['class'] == 0]))\n",
    "print('Number of fast progressors in Train set:',len(x_train.loc[x_train['class'] == 1]))\n",
    "\n",
    "print(y_train.shape)\n",
    "print(x_test.shape)\n",
    "print('Number of slow progressors in Test set:',len(x_test.loc[x_test['class'] == 0]))\n",
    "print('Number of slow progressors in Test set:',len(x_test.loc[x_test['class'] == 1]))\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "x_train.to_csv('/srv/Normalized_numeric_data/Numeric_Train.csv')\n",
    "x_test.to_csv('/srv/Normalized_numeric_data/Numeric_Test.csv')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Binary classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "x_train = x_train.drop(columns={'class'})\n",
    "x_test = x_test.drop(columns={'class'})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## print most important features \n",
    "\n",
    "from xgboost import XGBRegressor\n",
    "xgbr = XGBRegressor(n_estimators=100)\n",
    "xgbr.fit(x_train, y_train)\n",
    "\n",
    "plt.barh(x_train.columns, xgbr.feature_importances_)\n",
    "#plt.show()\n",
    "print(x_train.columns)\n",
    "print(xgbr.feature_importances_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grid search for hyperparameters\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "tx = 0.4\n",
    "for lr in {0.1,0.2,0.4}:\n",
    "    for max_depth in {8, 32, 64, 128}:\n",
    "        for alpha in {0.1,0.2,0.5, 0.9}:\n",
    "            for gamma in {0.2,0.5,0.7,0.9}:\n",
    "        \n",
    "                for max_leaves in {4,8, 16}:\n",
    "                    params = {\n",
    "                        'num_rounds':        50,\n",
    "                        'max_depth':         max_depth,\n",
    "                        'max_leaves':        2**max_leaves,\n",
    "                        'alpha':             alpha,\n",
    "                        'eta':               0.1,\n",
    "                        'gamma':             gamma,\n",
    "                        'learning_rate':     lr,\n",
    "                        'subsample':         1,\n",
    "                        'reg_lambda':        1,\n",
    "                        'scale_pos_weight':  2,\n",
    "                        'tree_method':       'gpu_hist',\n",
    "                        'objective':         'binary:logistic',\n",
    "                        'verbose':           True\n",
    "                    }\n",
    "\n",
    "                    dtrain = xgb.DMatrix(x_train, label=y_train)\n",
    "                    dtest = xgb.DMatrix(x_test, label=y_test)\n",
    "                    evals = [(dtest, 'test',), (dtrain, 'train')]\n",
    "\n",
    "                    num_rounds = params['num_rounds']\n",
    "\n",
    "                    model = xgb.train(params, dtrain, num_rounds, evals=evals)\n",
    "\n",
    "                    # evaluation\n",
    "                    threshold = .5\n",
    "                    true_labels = y_test.astype(int)\n",
    "                    preds = model.predict(dtest)\n",
    "                    fpr, tpr, t = roc_curve(true_labels, preds)\n",
    "                    au = auc(fpr,tpr)\n",
    "\n",
    "                    if au > tx:\n",
    "                        tx = au\n",
    "                        best_lr = lr\n",
    "                        best_depth = max_depth\n",
    "                        best_leaves = max_leaves\n",
    "                        \n",
    "                        best_alpha = alpha\n",
    "                        best_gamma = gamma\n",
    "                        print(tx)\n",
    "\n",
    "print(tx, best_lr, best_depth, best_leaves, best_alpha, best_gamma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    'num_rounds':        75,\n",
    "    'max_depth':         8,\n",
    "    'max_leaves':        2**4,\n",
    "    'alpha':             0.1,\n",
    "    'eta':               0.1,\n",
    "    'gamma':             0.9,\n",
    "    'learning_rate':     0.1,\n",
    "    'subsample':         1,\n",
    "    'reg_lambda':        1,\n",
    "    'scale_pos_weight':  2,\n",
    "    'tree_method':       'gpu_hist',\n",
    "    'objective':         'binary:logistic',\n",
    "    'verbose':           True\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time \n",
    "\n",
    "dtrain = xgb.DMatrix(x_train, label=y_train)\n",
    "dtest = xgb.DMatrix(x_test, label=y_test)\n",
    "evals = [(dtest, 'test',), (dtrain, 'train')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_rounds = params['num_rounds']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = xgb.train(params, dtrain, num_rounds, evals=evals)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing and Evalution "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve, auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = .47\n",
    "true_labels = y_test.astype(int)\n",
    "true_labels.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = model.predict(dtest)\n",
    "\n",
    "pred_labels = (preds > threshold).astype(int)\n",
    "print(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_labels.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fpr, tpr, t = roc_curve(true_labels, preds)\n",
    "auc(fpr,tpr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = confusion_matrix(true_labels, pred_labels) \n",
    "\n",
    "def plot_confusion_matrix(cm, target_names, title='Confusion Matrix', cmap=plt.cm.Greens):\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(target_names))\n",
    "    plt.xticks(tick_marks, target_names, rotation=45)\n",
    "    plt.yticks(tick_marks, target_names)\n",
    "    plt.tight_layout()\n",
    "\n",
    "    width, height = cm.shape\n",
    "\n",
    "    for x in range(width):\n",
    "        for y in range(height):\n",
    "            plt.annotate(str(cm[x][y]), xy=(y, x), \n",
    "                        horizontalalignment='center',\n",
    "                        verticalalignment='center')\n",
    "    plt.ylabel('True Label')\n",
    "    plt.xlabel('Predicted Label')\n",
    "\n",
    "\n",
    "plot_confusion_matrix(results, ['slow','fast'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.ylim((0,1))\n",
    "plt.xlim((0,1))\n",
    "plt.plot(fpr,tpr)\n",
    "plt.plot([0,1],[0,1])\n",
    "plt.xlabel('false positive rate')\n",
    "plt.ylabel('true positive rate')\n",
    "plt.plot(fpr,t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "384px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
